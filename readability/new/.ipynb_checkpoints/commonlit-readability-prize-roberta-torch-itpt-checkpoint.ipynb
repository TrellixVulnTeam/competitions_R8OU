{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Pre-training\n",
    "\n",
    "Besides the training data of a target task, we can further pre-train a transformer on the data from the same domain.\n",
    "\n",
    "![image.png](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_16/MediaObjects/489562_1_En_16_Fig1_HTML.png)\n",
    "\n",
    "The Transformer models are pre-trained on the general domain corpus. For a text classification task / regression task in a specific domain, such as Readability Assesment, its data\n",
    "distribution may be different from a transformer trained on a different corpus e.g. RoBERTa trained on BookCorpus, Wiki, CC-News, OpenWebText, Stories. Therefore the idea is, we can further pre-train the transformer with masked language model and next sentence prediction tasks on the domain-specific data. Three further pretraining approaches are performed:\n",
    "\n",
    "1) `Within-task pre-training (ITPT)`, in which transformer is further pre-trained on the training data of a target task. `This Kernel.`\n",
    "\n",
    "2) `In-domain pre-training (IDPT)`, in which the pretraining data is obtained from the same domain of a target task. For example, there are several different sentiment classification tasks, which have a similar data distribution. We can further pre-train the transformer on the combined training data from these tasks.\n",
    "\n",
    "3) `Cross-domain pre-training (CDPT)`, in which the pretraining data is obtained from both the same and other different domains to a target task.\n",
    "\n",
    "#### Reference: [How to finetune BERT for Text Classification ?](https://arxiv.org/pdf/1905.05583.pdf)\n",
    "\n",
    "> Note: This Kernel implements ITPT i.e. Within-Task Pretraining. First we will pretrain a RoBERTa model and then utilize the same for further finetuing tasks using different strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Reference: \n",
    "`Transformer Examples` - https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm_no_trainer.py\n",
    "\n",
    "> 90-95% of the code is from this great `run_mlm_no_trainer.py script` from `HuggingFace Examples Repository`. I have merely `changed few lines to adjust the code according to my task`. \n",
    "    \n",
    "    P.S. Make sure to understand everything instead of blindly copying the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:29.298883Z",
     "iopub.status.busy": "2021-05-21T19:09:29.298545Z",
     "iopub.status.idle": "2021-05-21T19:09:40.807034Z",
     "shell.execute_reply": "2021-05-21T19:09:40.806229Z",
     "shell.execute_reply.started": "2021-05-21T19:09:29.298854Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CommonLit Readability Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:40.810486Z",
     "iopub.status.busy": "2021-05-21T19:09:40.810133Z",
     "iopub.status.idle": "2021-05-21T19:09:41.276891Z",
     "shell.execute_reply": "2021-05-21T19:09:41.276125Z",
     "shell.execute_reply.started": "2021-05-21T19:09:40.810452Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
    "test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "\n",
    "mlm_data = train[['excerpt']]\n",
    "mlm_data = mlm_data.rename(columns={'excerpt':'text'})\n",
    "mlm_data.to_csv('mlm_data.csv', index=False)\n",
    "\n",
    "mlm_data_val = test[['excerpt']]\n",
    "mlm_data_val = mlm_data_val.rename(columns={'excerpt':'text'})\n",
    "mlm_data_val.to_csv('mlm_data_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:41.279867Z",
     "iopub.status.busy": "2021-05-21T19:09:41.279483Z",
     "iopub.status.idle": "2021-05-21T19:09:48.552546Z",
     "shell.execute_reply": "2021-05-21T19:09:48.55181Z",
     "shell.execute_reply.started": "2021-05-21T19:09:41.279831Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING, \n",
    "    MODEL_MAPPING, \n",
    "    AdamW, \n",
    "    AutoConfig, \n",
    "    AutoModelForMaskedLM, \n",
    "    AutoTokenizer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    SchedulerType, \n",
    "    get_scheduler, \n",
    "    set_seed\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(MODEL_TYPES, width=3, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:48.554259Z",
     "iopub.status.busy": "2021-05-21T19:09:48.553933Z",
     "iopub.status.idle": "2021-05-21T19:09:48.56673Z",
     "shell.execute_reply": "2021-05-21T19:09:48.565628Z",
     "shell.execute_reply.started": "2021-05-21T19:09:48.554228Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    train_file= 'mlm_data.csv'\n",
    "    validation_file = 'mlm_data.csv'\n",
    "    validation_split_percentage= 5\n",
    "    pad_to_max_length= True\n",
    "    model_name_or_path= 'roberta-base'\n",
    "    config_name= 'roberta-base'\n",
    "    tokenizer_name= 'roberta-base'\n",
    "    use_slow_tokenizer= True\n",
    "    per_device_train_batch_size= 8\n",
    "    per_device_eval_batch_size= 8\n",
    "    learning_rate= 5e-5\n",
    "    weight_decay= 0.0\n",
    "    num_train_epochs= 1 # change to 5\n",
    "    max_train_steps= None\n",
    "    gradient_accumulation_steps= 1\n",
    "    lr_scheduler_type= 'constant_with_warmup'\n",
    "    num_warmup_steps= 0\n",
    "    output_dir= 'output'\n",
    "    seed= 2021\n",
    "    model_type= 'roberta'\n",
    "    max_seq_length= None\n",
    "    line_by_line= False\n",
    "    preprocessing_num_workers= 4\n",
    "    overwrite_cache= True\n",
    "    mlm_probability= 0.15\n",
    "\n",
    "config = TrainConfig()\n",
    "\n",
    "if config.train_file is not None:\n",
    "    extension = config.train_file.split(\".\")[-1]\n",
    "    assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, json or txt file.\"\n",
    "if config.validation_file is not None:\n",
    "    extension = config.validation_file.split(\".\")[-1]\n",
    "    assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, json or txt file.\"\n",
    "if config.output_dir is not None:\n",
    "    os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:48.569506Z",
     "iopub.status.busy": "2021-05-21T19:09:48.569141Z",
     "iopub.status.idle": "2021-05-21T19:09:48.60221Z",
     "shell.execute_reply": "2021-05-21T19:09:48.601616Z",
     "shell.execute_reply.started": "2021-05-21T19:09:48.569469Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = TrainConfig()\n",
    "    accelerator = Accelerator()\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state)\n",
    "    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = args.train_file.split(\".\")[-1]\n",
    "    if extension == \"txt\":\n",
    "        extension = \"text\"\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "    \n",
    "    if args.config_name:\n",
    "        config = AutoConfig.from_pretrained(args.config_name)\n",
    "    elif config.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    if args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "    elif args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "    \n",
    "    if args.model_name_or_path:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "            config=config,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "    if args.max_seq_length is None:\n",
    "        max_seq_length = tokenizer.model_max_length\n",
    "        if max_seq_length > 1024:\n",
    "            logger.warning(\n",
    "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "            )\n",
    "            max_seq_length = 1024\n",
    "    else:\n",
    "        if args.max_seq_length > tokenizer.model_max_length:\n",
    "            logger.warning(\n",
    "                f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "            )\n",
    "        max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "    )\n",
    "\n",
    "    def group_texts(examples):\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        total_length = (total_length // max_seq_length) * max_seq_length\n",
    "        result = {\n",
    "            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "    )\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    else:\n",
    "        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n",
    "\n",
    "    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n",
    "\n",
    "        losses = torch.cat(losses)\n",
    "        losses = losses[: len(eval_dataset)]\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "\n",
    "        logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T19:09:48.604627Z",
     "iopub.status.busy": "2021-05-21T19:09:48.604178Z",
     "iopub.status.idle": "2021-05-21T19:13:34.776786Z",
     "shell.execute_reply": "2021-05-21T19:13:34.77579Z",
     "shell.execute_reply.started": "2021-05-21T19:09:48.604571Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
