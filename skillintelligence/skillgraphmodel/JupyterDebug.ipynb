{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "import dgl\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import dgl.function as fn\n",
    "from IPython.core.debugger import set_trace \n",
    "\n",
    "def disable_grad(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def compact_and_copy(frontier, seeds):\n",
    "    block = dgl.to_block(frontier, seeds)\n",
    "    for col, data in frontier.edata.items():\n",
    "        if col == dgl.EID:\n",
    "            continue\n",
    "        block.edata[col] = data[block.edata[dgl.EID]]\n",
    "    return block\n",
    "\n",
    "def _init_input_modules(g, ntype, hidden_dims):\n",
    "    # We initialize the linear projections of each input feature ``x`` as\n",
    "    # follows:\n",
    "    # * If ``x`` is a scalar integral feature, we assume that ``x`` is a categorical\n",
    "    #   feature, and assume the range of ``x`` is 0..max(x).\n",
    "    # * If ``x`` is a float one-dimensional feature, we assume that ``x`` is a\n",
    "    #   numeric vector.\n",
    "    # * If ``x`` is a field of a textset, we process it as bag of words.\n",
    "    module_dict = nn.ModuleDict()\n",
    "\n",
    "    for column, data in g.nodes[ntype].data.items():\n",
    "        if column == dgl.NID:\n",
    "            continue\n",
    "        if data.dtype == torch.float32:\n",
    "            assert data.ndim == 2\n",
    "            m = nn.Linear(data.shape[1], hidden_dims)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            module_dict[column] = m\n",
    "        elif data.dtype == torch.int64:\n",
    "            assert data.ndim == 1\n",
    "            m = nn.Embedding(\n",
    "                data.max() + 2, hidden_dims, padding_idx=-1)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            module_dict[column] = m\n",
    "    return module_dict\n",
    "\n",
    "class BagOfWordsPretrained(nn.Module):\n",
    "    def __init__(self, field, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        input_dims = field.vocab.vectors.shape[1]\n",
    "        self.emb = nn.Embedding(\n",
    "            len(field.vocab.itos), input_dims,\n",
    "            padding_idx=field.vocab.stoi[field.pad_token])\n",
    "        self.emb.weight[:] = field.vocab.vectors\n",
    "        self.proj = nn.Linear(input_dims, hidden_dims)\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "        nn.init.constant_(self.proj.bias, 0)\n",
    "\n",
    "        disable_grad(self.emb)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        \"\"\"\n",
    "        x: (batch_size, max_length) LongTensor\n",
    "        length: (batch_size,) LongTensor\n",
    "        \"\"\"\n",
    "        x = self.emb(x).sum(1) / length.unsqueeze(1).float()\n",
    "        return self.proj(x)\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, field, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(\n",
    "            len(field.vocab.itos), hidden_dims,\n",
    "            padding_idx=field.vocab.stoi[field.pad_token])\n",
    "        nn.init.xavier_uniform_(self.emb.weight)\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        return self.emb(x).sum(1) / length.unsqueeze(1).float()\n",
    "\n",
    "class LinearProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects each input feature of the graph linearly and sums them up\n",
    "    \"\"\"\n",
    "    def __init__(self, full_graph, ntype, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ntype = ntype\n",
    "        self.inputs = _init_input_modules(full_graph, ntype, hidden_dims)\n",
    "\n",
    "    def forward(self, ndata):\n",
    "        projections = []\n",
    "        for feature, data in ndata.items():\n",
    "            if feature == dgl.NID or feature.endswith('__len'):\n",
    "              # This is an additional feature indicating the length of the ``feature``\n",
    "              # column; we shouldn't process this.\n",
    "                continue\n",
    "\n",
    "            module = self.inputs[feature]\n",
    "            if isinstance(module, (BagOfWords, BagOfWordsPretrained)):\n",
    "              # Textual feature; find the length and pass it to the textual module.\n",
    "                length = ndata[feature + '__len']\n",
    "                result = module(data, length)\n",
    "            else:\n",
    "                result = module(data)\n",
    "            projections.append(result)\n",
    "\n",
    "        return torch.stack(projections, 1).sum(1)\n",
    "\n",
    "class WeightedSAGEConv(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, act=F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = act\n",
    "        self.Q = nn.Linear(input_dims, hidden_dims)\n",
    "        self.W = nn.Linear(input_dims + hidden_dims, output_dims)\n",
    "        self.reset_parameters()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.Q.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.W.weight, gain=gain)\n",
    "        nn.init.constant_(self.Q.bias, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "    def forward(self, g, h, weights):\n",
    "        \"\"\"\n",
    "        g : graph\n",
    "        h : node features\n",
    "        weights : scalar edge weights\n",
    "        \"\"\"\n",
    "        h_src, h_dst = h\n",
    "        with g.local_scope():\n",
    "            g.srcdata['n'] = self.act(self.Q(self.dropout(h_src)))\n",
    "            g.edata['w'] = weights.float()\n",
    "            g.update_all(fn.u_mul_e('n', 'w', 'm'), fn.sum('m', 'n'))\n",
    "            g.update_all(fn.copy_e('w', 'm'), fn.sum('m', 'ws'))\n",
    "            n = g.dstdata['n']\n",
    "            ws = g.dstdata['ws'].unsqueeze(1).clamp(min=1)\n",
    "            z = self.act(self.W(self.dropout(torch.cat([n / ws, h_dst], 1))))\n",
    "            z_norm = z.norm(2, 1, keepdim=True)\n",
    "            z_norm = torch.where(z_norm == 0, torch.tensor(1.).to(z_norm), z_norm)\n",
    "            z = z / z_norm\n",
    "            return z\n",
    "\n",
    "class SAGENet(nn.Module):\n",
    "    def __init__(self, hidden_dims, n_layers):\n",
    "        \"\"\"\n",
    "        g : DGLHeteroGraph\n",
    "            The user-item interaction graph.\n",
    "            This is only for finding the range of categorical variables.\n",
    "        item_textsets : torchtext.data.Dataset\n",
    "            The textual features of each item node.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.convs.append(WeightedSAGEConv(hidden_dims, hidden_dims, hidden_dims))\n",
    "\n",
    "    def forward(self, blocks, h):\n",
    "        for layer, block in zip(self.convs, blocks):\n",
    "            h_dst = h[:block.number_of_nodes('DST/' + block.ntypes[0])]\n",
    "            h = layer(block, (h, h_dst), block.edata['weights'])\n",
    "        return h\n",
    "\n",
    "class ItemToItemScorer(nn.Module):\n",
    "    def __init__(self, full_graph, ntype):\n",
    "        super().__init__()\n",
    "\n",
    "        n_nodes = full_graph.number_of_nodes(ntype)\n",
    "        self.bias = nn.Parameter(torch.zeros(n_nodes))\n",
    "\n",
    "    def _add_bias(self, edges):\n",
    "        bias_src = self.bias[edges.src[dgl.NID]]\n",
    "        bias_dst = self.bias[edges.dst[dgl.NID]]\n",
    "        return {'s': edges.data['s'] + bias_src + bias_dst}\n",
    "\n",
    "    def forward(self, item_item_graph, h):\n",
    "        \"\"\"\n",
    "        item_item_graph : graph consists of edges connecting the pairs\n",
    "        h : hidden state of every node\n",
    "        \"\"\"\n",
    "        with item_item_graph.local_scope():\n",
    "            item_item_graph.ndata['h'] = h\n",
    "            item_item_graph.apply_edges(fn.u_dot_v('h', 'h', 's'))\n",
    "            item_item_graph.apply_edges(self._add_bias)\n",
    "            pair_score = item_item_graph.edata['s']\n",
    "        return pair_score\n",
    "\n",
    "class PinSAGEModel(nn.Module):\n",
    "    def __init__(self, full_graph, ntype, hidden_dims, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = LinearProjector(full_graph, ntype, hidden_dims)\n",
    "        self.sage = SAGENet(hidden_dims, n_layers)\n",
    "        self.scorer = ItemToItemScorer(full_graph, ntype)\n",
    "\n",
    "    def forward(self, pos_graph, neg_graph, blocks):\n",
    "        h_item = self.get_repr(blocks)\n",
    "        pos_score = self.scorer(pos_graph, h_item)\n",
    "        neg_score = self.scorer(neg_graph, h_item)\n",
    "        return (neg_score - pos_score + 1).clamp(min=0)\n",
    "\n",
    "    def get_repr(self, blocks):\n",
    "        h_item = self.proj(blocks[0].srcdata)\n",
    "        h_item_dst = self.proj(blocks[-1].dstdata)\n",
    "        return h_item_dst + self.sage(blocks, h_item)\n",
    "\n",
    "args = {'random_walk_length':2,\n",
    "        'random_walk_restart_prob':0.5,\n",
    "        'num_random_walks':10,\n",
    "        'num_neighbors':3,\n",
    "        'num_layers':2,\n",
    "        'hidden_dims':64,\n",
    "        'batch_size':64,\n",
    "        'device': 'cpu',     #'cuda:0',\n",
    "        'num_epochs':10,\n",
    "        'batches_per_epoch':100, #20000\n",
    "        'num_workers':2,\n",
    "        'lr':3e-5,\n",
    "        'k':10,\n",
    "        'save_epochs':5,\n",
    "        'eval_epochs':5,\n",
    "        'retrain':0,\n",
    "        'save_path':'model_supervised',\n",
    "        }\n",
    "\n",
    "class ItemToItemBatchSampler(IterableDataset):\n",
    "    def __init__(self, g, user_type, item_type, batch_size, related_skill_dict):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.related_skill_dict = related_skill_dict\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            heads = torch.randint(0, self.g.number_of_nodes(self.item_type), (self.batch_size,))\n",
    "            set_trace()\n",
    "            tails = []\n",
    "            neg_tails = []\n",
    "\n",
    "            for h in heads.numpy():\n",
    "                related_S = self.related_skill_dict[h]\n",
    "                i = int(np.random.randint(0, len(related_S), size=1))\n",
    "                t = related_S[i]\n",
    "                n_i = int(np.random.randint(0, self.g.number_of_nodes(self.item_type), size=1))\n",
    "                while n_i in related_S:\n",
    "                    n_i = int(np.random.randint(0, self.g.number_of_nodes(self.item_type), size=1))\n",
    "                tails.append(t)\n",
    "                neg_tails.append(n_i)\n",
    "            tails = torch.as_tensor(tails)\n",
    "            neg_tails = torch.as_tensor(neg_tails)\n",
    "\n",
    "            mask = (tails != -1)\n",
    "            yield heads[mask], tails[mask], neg_tails[mask]\n",
    "\n",
    "\n",
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, user_type, item_type, random_walk_length, random_walk_restart_prob,\n",
    "                 num_random_walks, num_neighbors, num_layers):\n",
    "        self.g = g\n",
    "        self.user_type = user_type\n",
    "        self.item_type = item_type\n",
    "        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n",
    "        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n",
    "        self.samplers = [\n",
    "            dgl.sampling.PinSAGESampler(g, item_type, user_type, random_walk_length,\n",
    "                                        random_walk_restart_prob, num_random_walks, num_neighbors)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "    def sample_blocks(self, seeds, heads=None, tails=None, neg_tails=None):\n",
    "        blocks = []\n",
    "        for sampler in self.samplers:\n",
    "            frontier = sampler(seeds)\n",
    "            if heads is not None:\n",
    "                eids = frontier.edge_ids(torch.cat([heads, heads]), torch.cat([tails, neg_tails]), return_uv=True)[2]\n",
    "                set_trace()\n",
    "                if len(eids) > 0:\n",
    "                    old_frontier = frontier\n",
    "                    frontier = dgl.remove_edges(old_frontier, eids)\n",
    "                    # print(old_frontier)\n",
    "                    # print(frontier)\n",
    "                    # print(frontier.edata['weights'])\n",
    "                    # frontier.edata['weights'] = old_frontier.edata['weights'][frontier.edata[dgl.EID]]\n",
    "            block = compact_and_copy(frontier, seeds)\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "            blocks.insert(0, block)\n",
    "        return blocks\n",
    "\n",
    "    def sample_from_item_pairs(self, heads, tails, neg_tails):\n",
    "        # Create a graph with positive connections only and another graph with negative\n",
    "        # connections only.\n",
    "        pos_graph = dgl.graph(\n",
    "            (heads, tails),\n",
    "            num_nodes=self.g.number_of_nodes(self.item_type))\n",
    "        neg_graph = dgl.graph(\n",
    "            (heads, neg_tails),\n",
    "            num_nodes=self.g.number_of_nodes(self.item_type))\n",
    "        pos_graph, neg_graph = dgl.compact_graphs([pos_graph, neg_graph])\n",
    "        seeds = pos_graph.ndata[dgl.NID]\n",
    "\n",
    "        blocks = self.sample_blocks(seeds, heads, tails, neg_tails)\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "\n",
    "def assign_simple_node_features(ndata, g, ntype, assign_id=False):\n",
    "    \"\"\"\n",
    "    Copies data to the given block from the corresponding nodes in the original graph.\n",
    "    \"\"\"\n",
    "    for col in g.nodes[ntype].data.keys():\n",
    "        if not assign_id and col == dgl.NID:\n",
    "            continue\n",
    "        induced_nodes = ndata[dgl.NID]\n",
    "        ndata[col] = g.nodes[ntype].data[col][induced_nodes]\n",
    "\n",
    "def assign_features_to_blocks(blocks, g, ntype):\n",
    "    # For the first block (which is closest to the input), copy the features from\n",
    "    # the original graph as well as the texts.\n",
    "    assign_simple_node_features(blocks[0].srcdata, g, ntype)\n",
    "    # assign_textual_node_features(blocks[0].srcdata, textset, ntype)\n",
    "    assign_simple_node_features(blocks[-1].dstdata, g, ntype)\n",
    "    # assign_textual_node_features(blocks[-1].dstdata, textset, ntype)\n",
    "\n",
    "class PinSAGECollator(object):\n",
    "    def __init__(self, sampler, g, ntype):\n",
    "        self.sampler = sampler\n",
    "        self.ntype = ntype\n",
    "        self.g = g\n",
    "\n",
    "    def collate_train(self, batches):\n",
    "        heads, tails, neg_tails = batches[0]\n",
    "        # Construct multilayer neighborhood via PinSAGE...\n",
    "        pos_graph, neg_graph, blocks = self.sampler.sample_from_item_pairs(heads, tails, neg_tails)\n",
    "        assign_features_to_blocks(blocks, self.g, self.ntype)\n",
    "\n",
    "        return pos_graph, neg_graph, blocks\n",
    "\n",
    "    def collate_test(self, samples):\n",
    "        batch = torch.LongTensor(samples)\n",
    "        blocks = self.sampler.sample_blocks(batch)\n",
    "        assign_features_to_blocks(blocks, self.g, self.ntype)\n",
    "        return blocks\n",
    "\n",
    "def prepareDataSet():\n",
    "    with open('save_posting_whole_sup.p', 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "def prepareDataLoader(dataset, args):\n",
    "    g = dataset['train-graph']\n",
    "    val_matrix = dataset['val-matrix'].tocsr()\n",
    "    test_matrix = dataset['test-matrix'].tocsr()\n",
    "    item_texts = dataset['item-texts']\n",
    "    user_ntype = dataset['user-type']\n",
    "    item_ntype = dataset['item-type']\n",
    "    user_to_item_etype = dataset['user-to-item-type']\n",
    "    related_skill = dataset['related-skill']\n",
    "\n",
    "    device = torch.device(args['device'])\n",
    "\n",
    "    # Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "    # embedding for each entity)\n",
    "    g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n",
    "    g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))\n",
    "\n",
    "    # Sampler\n",
    "    batch_sampler = ItemToItemBatchSampler(\n",
    "        g, user_ntype, item_ntype, args['batch_size'], related_skill)\n",
    "    neighbor_sampler = NeighborSampler(\n",
    "        g, user_ntype, item_ntype, args['random_walk_length'],\n",
    "        args['random_walk_restart_prob'], args['num_random_walks'], args['num_neighbors'],\n",
    "        args['num_layers'])\n",
    "    collator = PinSAGECollator(neighbor_sampler, g, item_ntype)\n",
    "    dataloader = DataLoader(\n",
    "        batch_sampler,\n",
    "        collate_fn=collator.collate_train,\n",
    "        num_workers=args['num_workers'])\n",
    "    dataloader_test = DataLoader(\n",
    "        torch.arange(g.number_of_nodes(item_ntype)),\n",
    "        batch_size=args['batch_size'],\n",
    "        collate_fn=collator.collate_test,\n",
    "        num_workers=args['num_workers'])\n",
    "    dataloader_it = iter(dataloader)\n",
    "\n",
    "    return g, dataloader_it\n",
    "\n",
    "def createModel(g, item_ntype):\n",
    "    device = args['device']\n",
    "    model = PinSAGEModel(g, item_ntype, args['hidden_dims'], args['num_layers']).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    start_epoch = 0\n",
    "\n",
    "    return model, opt\n",
    "\n",
    "def train(dataloader_it, model):\n",
    "    # For each batch of head-tail-negative triplets...\n",
    "    device = args['device']\n",
    "    for epoch_id in tqdm(range(0, args['num_epochs'])):\n",
    "        #model.train()\n",
    "        for batch_id in tqdm(range(args['batches_per_epoch'])):\n",
    "            pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "            #print(\"pos_graph \", pos_graph)\n",
    "            #print(\"neg_graph \", neg_graph)\n",
    "            #print(\"blocks \", blocks)\n",
    "            # Copy to GPU\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"This is the start of PinSage debug Test\")\n",
    "    dataSet = prepareDataSet()\n",
    "    #model, opt = createModel(dataSet['train-graph'], dataSet['item-type'])\n",
    "    g, dataloader = prepareDataLoader(dataSet, args)\n",
    "\n",
    "    train(dataloader, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678b7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
